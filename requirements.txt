--extra-index-url https://download.pytorch.org/whl/cu128
# PyTorch with CUDA 12.8
safetensors==0.7.0
torch==2.8.0+cu128; sys_platform == 'win32'
torchaudio==2.8.0+cu128; sys_platform == 'win32'

# Core dependencies
transformers>=4.51.0,<4.58.0
diffusers==0.31.0
gradio==6.2.0
matplotlib>=3.7.5
scipy>=1.10.1
soundfile>=0.13.1
loguru>=0.7.3
einops>=0.8.1
accelerate>=1.12.0
fastapi>=0.110.0
uvicorn[standard]>=0.27.0
numba>=0.63.1
vector-quantize-pytorch>=1.27.15
torchcodec>=0.9.1; sys_platform != 'darwin'
torchao==0.15.0
toml
modelscope

# Training dependencies (required for LoRA/LoKr training)
peft>=0.18.0
lycoris-lora
lightning>=2.0.0
tensorboard>=2.0.0

# nano-vllm dependencies
triton-windows>=3.0.0,<3.4; sys_platform == 'win32'
flash-attn @ https://github.com/sdbds/flash-attention-for-windows/releases/download/2.8.2/flash_attn-2.8.2+cu128torch2.7.1cxx11abiFALSEfullbackward-cp311-cp311-win_amd64.whl
xxhash

# Local package - install with: pip install -e acestep/third_parts/nano-vllm
./acestep/third_parts/nano-vllm

# Required for Side-Step CLI
rich>=13.0.0

# Required for Side-Step TUI
textual>=0.47.0

# 8-bit optimizers (saves ~30-40% optimizer VRAM)
# AdamW8bit in the optimizer selector.
bitsandbytes>=0.45.0

# Prodigy adaptive optimizer (auto-tunes LR)
# Great if you don't want to manually tune learning rate.
prodigyopt>=1.1.2